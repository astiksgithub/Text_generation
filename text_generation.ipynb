{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import dependencies\n",
    "import numpy\n",
    "import sys\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "file = open(\"frankenstein-2.txt\", encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "#standardization\n",
    "def tokensize_words(input):\n",
    "    input = input.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(input)\n",
    "    filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
    "    return ' '.join(filtered)\n",
    "#preprocess the input data\n",
    "processed_inputs = tokensize_words(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chars to numbers\n",
    "#convert characters in our input to numbers\n",
    "#we'll sort the list of the set of all characters that appear in out i/p textand then use the enumerate in to get numbers\n",
    "#that represent the characters\n",
    "#we'll then create a dictionary that stores the keys and values, or the characters and the numbers that represent them\n",
    "chars = sorted(list(set(processed_inputs)))\n",
    "char_to_num = dict((c,i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters:  269913\n",
      "Total vocab:  42\n"
     ]
    }
   ],
   "source": [
    "#check if words to chars or chars to num(?!) has worked?\n",
    "#just so we get an idea of whether our process of converting words to characters has worked\n",
    "#we print the length of our variables\n",
    "input_len = len(processed_inputs)\n",
    "vocab_len = len(chars)\n",
    "print(\"Total number of characters: \", input_len)\n",
    "print(\"Total vocab: \", vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq length\n",
    "# wer're defining how long we want an individual sequence here\n",
    "#an individual sequence is a complete mapping of input characters as integers\n",
    "seq_length = 100\n",
    "x_data = []\n",
    "y_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  269813\n"
     ]
    }
   ],
   "source": [
    "#loop through the sequence\n",
    "#here we're going through the entire list of i/ps and converting the chars to numbers with a for loop\n",
    "#this will create a bunch of sequence where each sequence starts with the next character in the i/p data\n",
    "#beginning with the first character\n",
    "for i in range(0, input_len - seq_length, 1):\n",
    "    #define i/p and o/p sequence\n",
    "    in_seq = processed_inputs[i:i + seq_length]\n",
    "    out_seq = processed_inputs[i + seq_length]\n",
    "    x_data.append([char_to_num[char] for char in in_seq])\n",
    "    y_data.append([char_to_num[out_seq]])\n",
    "\n",
    "n_patterns = len(x_data)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert input to np array\n",
    "X = numpy.reshape(x_data, (n_patterns, seq_length, 1))\n",
    "X = X/float(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encoding our label data\n",
    "y = to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the model\n",
    "#creating a sequential model\n",
    "#dropout is used to prevent overfitting\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.LSTM(256, return_sequences=True))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.LSTM(128))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(y.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving weights\n",
    "filepath = \"model_weights_saved.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose = 1, save_best_only=True, mode='min')\n",
    "desired_callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1054/1054 [==============================] - ETA: 0s - loss: 2.9127\n",
      "Epoch 00001: loss improved from inf to 2.91273, saving model to model_weights_saved.hdf5\n",
      "1054/1054 [==============================] - 162s 154ms/step - loss: 2.9127\n",
      "Epoch 2/4\n",
      "1054/1054 [==============================] - ETA: 0s - loss: 2.6408\n",
      "Epoch 00002: loss improved from 2.91273 to 2.64076, saving model to model_weights_saved.hdf5\n",
      "1054/1054 [==============================] - 162s 154ms/step - loss: 2.6408\n",
      "Epoch 3/4\n",
      "1054/1054 [==============================] - ETA: 0s - loss: 2.4940\n",
      "Epoch 00003: loss improved from 2.64076 to 2.49404, saving model to model_weights_saved.hdf5\n",
      "1054/1054 [==============================] - 162s 154ms/step - loss: 2.4940\n",
      "Epoch 4/4\n",
      "1054/1054 [==============================] - ETA: 0s - loss: 2.3762\n",
      "Epoch 00004: loss improved from 2.49404 to 2.37620, saving model to model_weights_saved.hdf5\n",
      "1054/1054 [==============================] - 161s 153ms/step - loss: 2.3762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7c12a41978>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit model and let it train\n",
    "model.fit(X,y, epochs=4, batch_size=256, callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recompile model with the saved weights\n",
    "filename = \"model_weights_saved.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output of the model back into chaarcters\n",
    "num_to_char = dict((i,c) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed: \n",
      "\" divine day happy serene nature appears thus elizabeth endeavoured divert thoughts mine reflection up \"\n"
     ]
    }
   ],
   "source": [
    "#random seed to help generate \n",
    "start = numpy.random.randint(0, len(x_data) - 1)\n",
    "pattern = x_data[start]\n",
    "print('Random Seed: ')\n",
    "print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rk seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared seared sea"
     ]
    }
   ],
   "source": [
    "#genrate the text\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1,len(pattern), 1))\n",
    "    x = x/float(vocab_len)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = num_to_char[index]\n",
    "    seq_in = [num_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
